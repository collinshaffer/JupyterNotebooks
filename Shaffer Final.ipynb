{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Top Synonyms\n",
    "by Collin Shaffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original plan for this project was indeed rather vague, which eventually became my achilles heel. In order to have a program \"understand,\" it must be able to parse sentences, apply parts-of-speech to each word, determine word senses, and much more. What I was hoping to achieve was apparently the holy grail of a field where even the amateurs seem to have master's degrees. I did realize, only too late, that as I was writing code to anaylze the dictionary and thesaurus I had found, I was actually getting closer to a model for Word Sense Disambiguation. If I had used a corpus of sentences rather than a dictionary, perharps the results could have been more conclusive.\n",
    "\n",
    "Essentially, what I wrote a method that turns a list of words into a larger list of words (that are synonyms of the first list), takes the unique words amongst them, removes any that aren't in the list of defined roots, and returns the top $k$ of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As originally proposed, the data for the dictionary used is a taken from [this dataset](https://github.com/adambom/dictionary)  found on GitHub, uploaded by user `adambom`. The `dictionary.json` file contains a dictionary, in the form of a dictionary! (That is, of the format {\"word\":\"definition\"}.) The thesaurus data was taken from the [Moby Poroject](http://icon.shef.ac.uk/Moby/), an early 1990's natural language processing project by Grady Ward, now made available for free by [Project Gutenberg](https://www.gutenberg.org/files/3202/).\n",
    "\n",
    "The dictionary was read in using the eval function to keep the data as a dictionary. The thesaurus data was read in as a list via the csv package. Both the dictionary and the thesausrus were then individually separated into two sub-dictionaries based on keys that are either single words or phrases. \n",
    "\n",
    "The dictionary contains 83,684 defined single-word entries and 2,352 defined phrases, making a total of 86,036 definitions. The thesaurus contains 25,337 root words and 4,922 root phrases, totaling in 30,259. For simplicity, we will only be dealing with the single-word entries.\n",
    "\n",
    "Of the 83,684 single-word entries, 14,999, about 17.92%, are thesaurus roots. Of 1,157,056 total words used in definitions, 145,077 (12.54%) are unique and, of those, 17,403 (12.00%) are thesaurus roots. This means 17403/25337, or 68.69%, thesaurus roots are used in definitions.\n",
    "\n",
    " Of the single-word roots, 2,758,284 words are listed as synonyms with just 69,857 (2.53%) are unique. Yet 25,222 of those unique synonyms are also roots, meaning roughly 99.55% thesaurus roots are also synonyms for other roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictionaryAndThesaurus():\n",
    "    '''\n",
    "    Reads in the dictionary and the thesaurus, separates out the phrases\n",
    "    Returns two dictionaries {entry:definition} and two thesauruses {root:synonyms}, where for each, the keys are single words and multiple words respectively.\n",
    "    '''\n",
    "# def makeDictionaryData():\n",
    "    with open('dictionary.json') as dictFile:\n",
    "        dictionary = eval(dictFile.read())\n",
    "    #since dictionary is already a {}, first make pDictionary, then remove phrases from dictionary\n",
    "    pDictionary = {}\n",
    "    for k,v in dictionary.items():\n",
    "        if ' ' in k:\n",
    "            pDictionary[k] = v\n",
    "    for k in pDictionary.keys():\n",
    "        if k in dictionary.keys():\n",
    "            dictionary.pop(k)\n",
    "\n",
    "# def makeThesaurusData():\n",
    "    # simple fix - how to read in a csv with different length lines - http://www.gossamer-threads.com/lists/python/python/1151633\n",
    "    with open('mthesaur.txt') as file:\n",
    "        thesaurusData = list(csv.reader(file))\n",
    "        \n",
    "    # separate the phrase thesaurus & make both dictionaries\n",
    "    thesaurus = {}\n",
    "    pThesaurus = {}\n",
    "    for i in range(len(thesaurusData)):\n",
    "        ## assign sets \n",
    "        if ' ' in thesaurusData[i][0]:\n",
    "            pThesaurus[thesaurusData[i][0]] = thesaurusData[i][1:]\n",
    "        else:\n",
    "            thesaurus[thesaurusData[i][0]] = thesaurusData[i][1:]\n",
    "  \n",
    "    return dictionary, pDictionary, thesaurus, pThesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def getUniqueCharacters(dct):#, pDct, ths, pThs):\n",
    "    '''\n",
    "        This determined how many unique ascii characters are used in the definitions.\n",
    "        Necessary for minimizing the length of the character frequency array, as the alternate\n",
    "        length to generically cover the entire UTF-8 ascii character set would be 1,114,112!\n",
    "    '''\n",
    "    totalUniqueChars = set(())\n",
    "    entries = list(dct.keys())\n",
    "    defs = list(dct.values())\n",
    "#     pEntries= list(pDct.keys())\n",
    "#     pDefs = list(pDct.values())\n",
    "#Single-word\n",
    "    # defined singe words\n",
    "    for i in range(len(entries)):\n",
    "        temp = []\n",
    "        temp.extend(entries[i]) # turn string into char array\n",
    "        for c in temp:\n",
    "            totalUniqueChars.add(c)\n",
    "    # singe word defs\n",
    "    for i in range(len(defs)):\n",
    "        temp = []\n",
    "        temp.extend(defs[i]) # turn string into char array\n",
    "        for c in temp:\n",
    "            totalUniqueChars.add(c)\n",
    "\n",
    "#Phrase\n",
    "    # defined phrases\n",
    "#     for i in range(len(pEntries)):\n",
    "#         temp = []\n",
    "#         temp.extend(pEntries[i]) # turn string into char array\n",
    "#         for c in temp:\n",
    "#             totalUniqueChars.add(c)\n",
    "#     # phrase definitions   \n",
    "#     for i in range(len(pDefs)):\n",
    "#         temp = []\n",
    "#         temp.extend(pDefs[i]) # turn string into char array\n",
    "#         for c in temp:\n",
    "#             totalUniqueChars.add(c)\n",
    "\n",
    "    #print(len(totalUniqueChars))\n",
    "\n",
    "    nonalphanumerics = set(())\n",
    "    for c in totalUniqueChars:\n",
    "        if c != ' ' and c < '0' or (c > '9' and c < 'A') or (c > 'Z' and c < 'a') or c > 'z': #also not space\n",
    "            nonalphanumerics.add(c)\n",
    "\n",
    "    uchrs = list(totalUniqueChars)\n",
    "    uchrs.sort()\n",
    "    print('{} unique characters and {} nonalphanumeric characters (that aren\\'t space)'.format(len(uchrs), len(nonalphanumerics)))\n",
    "    return uchrs, nonalphanumerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 unique characters and 66 nonalphanumeric characters (that aren't space)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83684, 2352, 25337, 4922)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary, pDictionary, thesaurus, pThesaurus = getDictionaryAndThesaurus()\n",
    "uniqueCharacters, uniqueNonAlphaNumerics = getUniqueCharacters(dictionary)#, pDictionary, thesaurus, pThesaurus)\n",
    "len(dictionary), len(pDictionary), len(thesaurus), len(pThesaurus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14999 out of 83684 (17.92%) dictionary entries have synonyms.\n"
     ]
    }
   ],
   "source": [
    "entryWrdsWithSnnms = set(())\n",
    "for entry in dictionary.keys():\n",
    "    for char in uniqueNonAlphaNumerics:\n",
    "        entry = entry.strip(char)\n",
    "    if entry.lower() in thesaurus.keys():\n",
    "        entryWrdsWithSnnms.add(entry)\n",
    "        \n",
    "print('{} out of {} ({:.2f}%) dictionary entries have synonyms.'\n",
    "      .format(len(entryWrdsWithSnnms), len(dictionary.keys()), (len(entryWrdsWithSnnms)/len(dictionary.keys()))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1157051 total words used in definitions, 144862 (12.52%) are unique.\n",
      "Of those 144862, 17423 (12.03%) are thesaurus roots.\n",
      "This means 17423/25337, or 68.77%, thesaurus roots are used in definitions.\n"
     ]
    }
   ],
   "source": [
    "totalDefWrds = 0\n",
    "defWrdsWithSnnms = {}# essentially expand each dw into a dictionary of snnms\n",
    "theSetOfUniqueDefWords = set(())\n",
    "for defi in dictionary.values():#defi is a string\n",
    "    for wrdj in defi.split():#wrdj is the list of words in defi\n",
    "        for char in uniqueNonAlphaNumerics:\n",
    "            wrdj = wrdj.strip(char)\n",
    "        totalDefWrds += 1\n",
    "        if wrdj not in theSetOfUniqueDefWords:\n",
    "            theSetOfUniqueDefWords.add(wrdj)\n",
    "            if wrdj in thesaurus.keys():\n",
    "                defWrdsWithSnnms[wrdj] = thesaurus[wrdj]\n",
    "\n",
    "print('Out of {} total words used in definitions, {} ({:.2f}%) are unique.\\nOf those {}, {} ({:.2f}%) are thesaurus roots.'\n",
    "      .format(totalDefWrds, len(theSetOfUniqueDefWords), (len(theSetOfUniqueDefWords)/totalDefWrds)*100, \n",
    "              len(theSetOfUniqueDefWords), len(defWrdsWithSnnms), (len(defWrdsWithSnnms)/len(theSetOfUniqueDefWords))*100))\n",
    "print('This means {}/{}, or {:.2f}%, thesaurus roots are used in definitions.'\n",
    "      .format(len(defWrdsWithSnnms), len(thesaurus.keys()), (len(defWrdsWithSnnms)/len(thesaurus.keys()))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 2758284 words listed as synonyms, 69857 (2.53%) are unique.\n",
      "Of those 69857, 25222 (36.11%)are also roots.\n",
      "This means 25222/25337, or 99.55%, thesaurus roots are also synonyms.\n"
     ]
    }
   ],
   "source": [
    "theSetOfUniqueSnnms = set(())\n",
    "snnmsWithSnnms = set(())\n",
    "\n",
    "totalSnnms = 0\n",
    "\n",
    "for i in thesaurus.values():\n",
    "    for j in i:\n",
    "        if ' ' in j:\n",
    "            for k in j.split():\n",
    "                totalSnnms += 1\n",
    "                if k not in theSetOfUniqueSnnms:\n",
    "                    theSetOfUniqueSnnms.add(k)\n",
    "                    if k in thesaurus.keys():\n",
    "                        snnmsWithSnnms.add(k)\n",
    "        else:\n",
    "            totalSnnms += 1\n",
    "            if j not in theSetOfUniqueSnnms:\n",
    "                theSetOfUniqueSnnms.add(j)\n",
    "                if j in thesaurus.keys():\n",
    "                    snnmsWithSnnms.add(j)\n",
    "\n",
    "print('Out of {} words listed as synonyms, {} ({:.2f}%) are unique.\\nOf those {}, {} ({:.2f}%)are also roots.'\n",
    "      .format(totalSnnms, len(theSetOfUniqueSnnms), (len(theSetOfUniqueSnnms)/totalSnnms)*100, \n",
    "              len(theSetOfUniqueSnnms), len(snnmsWithSnnms), (len(snnmsWithSnnms)/len(theSetOfUniqueSnnms))*100))\n",
    "print('This means {}/{}, or {:.2f}%, thesaurus roots are also synonyms.'\n",
    "      .format(len(snnmsWithSnnms), len(thesaurus.keys()), (len(snnmsWithSnnms)/len(thesaurus.keys()))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Machine Learning Algoriths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Top Synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is, instead of using the euclidean distance to determine \"nearest\" neighbors as in KNN, each definition is first turned into a list of synonyms, then highest matching number of shared synonyms determines the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on the KNN class from lecture notebook 28\n",
    "'''\n",
    "######################################################################\n",
    "### class K Top Synonym\n",
    "######################################################################\n",
    "\n",
    "class KTS(object):\n",
    "    \n",
    "    def __init__(self, allLevels=False):\n",
    "        self.X = None  # data will be stored here\n",
    "        self.T = None  # class labels will be stored here\n",
    "        self.dictionary = {}\n",
    "        self.thesaurus = {}\n",
    "#         self.XdefWrdRoots = {}# essentially expand each dw into a dictionary of snnms\n",
    "        \n",
    "    def getRootsPerDef(self, X):\n",
    "        theSetOfUniqueDefWords = set(())\n",
    "        allRoots = []\n",
    "        for defi in X:#defi is a string\n",
    "            defRoots = []\n",
    "            for wrdj in defi.split():#wrdj is the list of words in defi\n",
    "                for char in uniqueNonAlphaNumerics:\n",
    "                    wrdj = wrdj.strip(char)\n",
    "                if wrdj not in theSetOfUniqueDefWords:\n",
    "                    theSetOfUniqueDefWords.add(wrdj)\n",
    "                    if wrdj in self.thesaurus.keys():\n",
    "                        defRoots += self.thesaurus[wrdj]\n",
    "            allRoots.append(defRoots)\n",
    "        return allRoots\n",
    "    \n",
    "    def train(self,X,T, dictionary,thesaurus):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.dictionary = dictionary\n",
    "        self.thesaurus = thesaurus\n",
    "#         self.XdefWrdsRoots = self.getRoots(self.X)\n",
    "        \n",
    "    def use(self, Xnew, k=1):\n",
    "        Xroots = self.getRootsPerDef(Xnew)\n",
    "        classes = []\n",
    "        for i in range(len(Xroots)):\n",
    "            uniqueDefRootsTest = [(w,int(f)) for w,f in np.array(np.unique(Xroots[i], return_counts=True) ).T]\n",
    "\n",
    "            uniqueDefRootsThatMacthTRoots = []\n",
    "            for i in uniqueDefRootsTest:\n",
    "                if i[0] in T:\n",
    "                    uniqueDefRootsThatMacthTRoots.append(i)\n",
    "\n",
    "            classesForThisRoot = []\n",
    "            for i in range(k):\n",
    "                maxFreqTst = ('',0)\n",
    "                for j in uniqueDefRootsThatMacthTRoots:\n",
    "                    if j[1] > maxFreqTst[1]:\n",
    "                        maxFreqTst = j\n",
    "                classesForThisRoot.append(maxFreqTst[0])\n",
    "                if maxFreqTst in uniqueDefRootsThatMacthTRoots:\n",
    "                    uniqueDefRootsThatMacthTRoots.remove(maxFreqTst)\n",
    "            classes.append(classesForThisRoot)\n",
    "        return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X will be the definitions for words that have thesaurus roots, and T will be those thesaurus roots. Xnew will be the remaining, or a subset of, definitions that will be classified based on the synonyms of the words in their definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14985, 14999, 68699, 14999, 83698)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i should be in dictionary.keys(), that was the condition for items being added to entryWrdsWithSnnms, \n",
    "# but this doesn't work without rechecking for some reason...\n",
    "X = []\n",
    "T = []\n",
    "for i in entryWrdsWithSnnms:\n",
    "    if i in dictionary.keys():\n",
    "        X.append(dictionary[i])\n",
    "    T.append(i.lower())\n",
    "Xnew = [dictionary[w] for w in set(dictionary.keys()).difference(entryWrdsWithSnnms)]\n",
    "len(X), len(T), len(Xnew), len(entryWrdsWithSnnms), len(Xnew)+len(entryWrdsWithSnnms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kts = KTS()\n",
    "kts.train(X,T,dictionary,thesaurus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classes = kts.use(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['order'],\n",
       " 'A tribute by the head; a capitation tax. [Written also chevageand chivage.] [Obs.]')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[444], Xnew[444]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of evaluating the results, since Xnew was actually chosen from words that <i>don't</i> have thesaurus roots, without partitioning X and T for Xnew so that we could actually compare accuracy, correctness is really just a matter of opinion. In short, what we have here are a couple of lists that let us determine how close the relationship is between a words and the most frequently occurring synonyms of the words in its definition from Webster's Unabridged English Dictionary. Personally think 'alarming' could be a synonym of 'awesomness', but without finding suplimental input, we may never know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHLOROPHYLL\t['ground']\n",
      "FUCHSIA   \t['cast']\n",
      "OATCAKE   \t['cereal']\n",
      "DISARMED  \t['grip']\n",
      "SURDINY   \t['']\n",
      "ALLOGENEOUS\t['air']\n",
      "AMNIOTA   \t['part']\n",
      "EWT       \t['']\n",
      "REREWARD  \t['train']\n",
      "CRESSY    \t['achromic']\n",
      "DISENCHAINED\t['abnegation']\n",
      "MIASMATIST\t['article']\n",
      "FEAZINGS  \t['line']\n",
      "DORRHAWK  \t['']\n",
      "-OUS      \t['essential']\n",
      "UT        \t['crack']\n",
      "TWELFTHTIDE\t['aeon']\n",
      "QUATRE    \t['jack']\n",
      "CHROMULE  \t['abstract']\n",
      "GREGORIAN \t['measure']\n"
     ]
    }
   ],
   "source": [
    "offset = 44\n",
    "for i in range(20):\n",
    "    for k,v in dictionary.items():\n",
    "        if v == Xnew[i+offset]:\n",
    "            print('{:10}\\t{}'.format(k,classes[i+offset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible applications that I didn't have time to explore were algorithms that followed the ideas behind support vector machines and clustering. A language-SVM could use a similar model to above, supplimented with antonyms, and/or going another level into the synonyms. If I had used a corpus of sentences rather than a dictionary, perharps the this method could have been used to determine words sense. For clustering, there is a resource called the Roget's Thesaurus, which groups synonyms together under meaning headings. This could have been very useful for the model herein, however, the only electronic Roget's Thesaures were very old, having many mistakes, not utf-8 encoded, and were not in an easily accessable form.\n",
    "\n",
    "While I may have been on the right path for a completing a small portion of the modern model for natural language processing, I fell short by underestimating how large even this first step would be.  However, I look forward to further developing this idea to determine if the same results could ever be achieved from a simple dictionary, e.g. maybe a kernal could be made for a natural language for fast interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
